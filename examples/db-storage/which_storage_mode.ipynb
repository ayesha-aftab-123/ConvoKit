{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will understand how to choose the best storage mode to use for your ConvoKit corpus: the original RAM based implementation, or the new database based storage mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oscarso/Desktop/Cornell-Conversational-Analysis-Toolkit/venv/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from convokit import Corpus, Speaker, Utterance, download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historically...\n",
    "Historically, ConvoKit allows you to work with conversational data directly in program memory through the Corpus class. Moreover, long term storage is provided by dumping the contents of a Corpus onto disk using the JSON format. This paradigm works well for distributing and storing static datasets, and for doing computations on conversational data that follow the pattern of doing computations on some or all of the data over a short time period and optionally storing these results in the permanent representation of the dataset. For example, ConvoKit distributes datasets included with the library in JSON format, which you can load into program memory to explore and compute with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /Users/oscarso/.convokit/downloads/reddit-corpus-small\n",
      "True\n",
      "Loading corpus None from disk at /Users/oscarso/.convokit/downloads/reddit-corpus-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 297132/297132 [00:01<00:00, 292082.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoModerator: Talk about your day. Anything goes, but subreddit rules still apply. Please be polite to each other! \n",
      "\n",
      "------------------------------\n",
      "belmont_lay: How to spoil a kpop fangirl's day. Tell her you want to send her a pic of g dragon.\n",
      "\n",
      "She'll be expecting something like [this](https://cdn2.i-scmp.com/sites/default/files/styles/landscape/public/images/methode/2018/01/18/482a92dc-fc0b-11e7-b2f7-03450b80c791_1280x720_135443.jpg?itok=tXZBSZ0u), but no. Send his [candid pics without makeup](https://koreaboo-cdn.storage.googleapis.com/2017/10/GDragon-Beard-01.jpg).\n",
      "\n",
      "Amazing what makeup can do for guys too, not just girls.\n",
      "------------------------------\n",
      "littlefiredragon: His \"candid\" pics look better leh\n",
      "------------------------------\n",
      "belmont_lay: wat.. he looks like a random extra in a JAV\n",
      "------------------------------\n",
      "rheinl: â€œHi Kpop girl can I send you a pic of g-dragon?â€\n",
      "\n",
      "â€œUhhh okâ€ (what the hell? this guy is so weird)\n",
      "\n",
      "â€œHere you goâ€ *sends pic of g-dragon with no makeup*\n",
      "\n",
      "â€œUhh thanks I guessâ€ (Jesus I hope he never talks to me again)\n",
      "------------------------------\n",
      "belmont_lay: I'm sorry if that's how your interactions with friends go ðŸ˜¥\n",
      "------------------------------\n",
      "rheinl: â€œHi Kpop girl can I send you a pic of g-dragonâ€\n",
      "\n",
      "â€œWoohoo I canâ€™t wait! Please send it right away!â€\n",
      "\n",
      "â€œHere you goâ€ *sends a pic of g-dragon with no make up*\n",
      "\n",
      "â€œUgh! No you spoilt my day! Hehe!â€\n",
      "------------------------------\n",
      "ThenPoem: Both ugly \n",
      "------------------------------\n",
      "sea_lecture: Bruh, I don't even like g-dragon but his comment is accurate af in describing how you'd come off as some socially awkward neckbeard if you talked like that to a girl.\n",
      "------------------------------\n",
      "brianne0007: Lmao ðŸ¤£ðŸ¤£\n",
      "------------------------------\n",
      "tauhuayislove: Ohhhh. Emmm. Geeeee. \n",
      "------------------------------\n",
      "[deleted]: [deleted]\n",
      "------------------------------\n",
      "belmont_lay: \"Bruh\", you're stupid if you think I talked to any of my friends in the manner he posted.\n",
      "\n",
      "I wrote a condensed version of what I did, not verbatim. \n",
      "\n",
      "End result was just people being surprised by how k pop idols look without make up, nothing awkward.\n",
      "\n",
      "I think people on this subreddit project too much when it comes to social awkwardness based on their own inadequacies.\n",
      "------------------------------\n",
      "belmont_lay: I'm sorry if that's how your interactions with friends go ðŸ˜¥\n",
      "\n",
      "But that's not how any of my whatsapps with my friends happened.\n",
      "------------------------------\n",
      "sea_lecture: &gt; if you think I talked to any of my friends in the manner he posted.\n",
      "\n",
      "Good thing I don't actually think that, but the fact that you're even fantasising and joking about it says a lot. \n",
      "\n",
      "Nah, you come off as a loser who doesn't know how to speak to women without scaring them off. That's a sad fact man. Now bye loser\n",
      "------------------------------\n",
      "dump to  /Users/oscarso/.convokit/saved-corpora/reddit-corpus-small\n",
      "True\n",
      "Loading corpus reddit-corpus-small from disk at /Users/oscarso/.convokit/saved-corpora/reddit-corpus-small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 297132/297132 [00:01<00:00, 276421.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# First we download the JSON files for the corpus from the ConvoKit servers, \n",
    "# if we don't already have a local copy. Then, we load the corpus into memory \n",
    "# by constructing a Corpus object and giving it the path to the corpus (as returned by download). \n",
    "reddit_small = Corpus(filename=download('reddit-corpus-small'), storage_type='mem')\n",
    "\n",
    "# Now we can easily work with the data through the corpus object to access the data within it.\n",
    "seen_utts = []\n",
    "for conversation in reddit_small.iter_conversations():\n",
    "    for utterance in conversation.iter_utterances():\n",
    "        print(f'{utterance.speaker.id}: {utterance.text}')\n",
    "        print('------------------------------')\n",
    "        utterance.meta['seen'] = True\n",
    "        seen_utts.append(utterance.id)\n",
    "    break\n",
    "    \n",
    "# Finally, to write the changes to the corpus (in this case, which utterances we saw in our test)\n",
    "# back to the JSON files so these changes are reflected in our local persistant representation of the corpus.\n",
    "reddit_small.dump(corpus_id='reddit-corpus-small')\n",
    "\n",
    "# We can confirm this by constructing a new corpus from the updated JSON files.\n",
    "reddit_small_2 = Corpus(corpus_id='reddit-corpus-small', storage_type='mem')\n",
    "for utterance in reddit_small_2.iter_utterances():\n",
    "    if utterance.id in seen_utts:\n",
    "        assert utterance.meta['seen'] == True\n",
    "    else:\n",
    "        assert 'seen' not in utterance.meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how in the above cell, we construct a corpus in two different ways. First, we use the  `filename` argument on line 4, and later use the `corpus_id` argument on lines 18 and 21. This is because we can consider a local download of a ConvoKit provided corpus as a locally cached version of the original corpus as it is distributed (i.e., we should not write to it directly). \n",
    "\n",
    "A downloaded corpus called `<corpus_id>` will by default be locally cached at `~/.convokit/downloads/<corpus_id>`; we use the `filename` paramater with download because download returns the full path to the corpus on disk. \n",
    "\n",
    "On the other hand, in general your local corpora live on disk in your `data_dir`: the directory specified in the configuration file at `~/.convokit/config.yml`; `data_dir` is `~/.convokit/saved-corpora` by default. Using the `corpus_id` paramater for initilization and to dump will read from/write to `<data_dir>/<corpus_id>`. You can also specify `data_dir` as an argument in `dump`, `download`, or a `Corpus` initilization to override the global default.\n",
    "\n",
    "Therefore, we use these two different ways to intilize a corpus to maintain the original, unaltered version of the Corpus at `~/.convokit/downloads/reddit-corpus-small`, while storing the version we are working with and modifying at `~/.convokit/saved-corpora/reddit-corpus-small`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In ConvoKit version (x.x.x)...\n",
    "In ConvoKit version (x.x.x), we introduce an new option for storing conversational data: Database storage. Consider a use case where you want to collect conversational data over a long time period and ensure you maintain a persistant representation of the dataset if your data collection program unexpectedly crashes. In the memory storage paradigm, this would require regularly dumping your corpus to JSON files, requiring repeated expensive write operations. On the other hand, with database storage all your data is automatically saved for long term storage in the database as it is added to the corpus. Lets view an example of constructing a corpus of reddit comments as they are posted, using ConvoKit alongside the praw wrapper library around the reddit API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from time import sleep\n",
    "\n",
    "# You can follow these instructions to get a client_id and client_secret to run this code yourself\n",
    "# https://www.geeksforgeeks.org/how-to-get-client_id-and-client_secret-for-python-reddit-api-registration/\n",
    "# (or, just view the output of running this code from before I removed my own credentials)\n",
    "reddit = praw.Reddit(client_id='<redacted>',\n",
    "                     client_secret='<redacted>',\n",
    "                     user_agent='name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to corpus reddit_live999_v0 in place in the DB\n",
      "ID: reddit_live999\n",
      "['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq']\n"
     ]
    }
   ],
   "source": [
    "reddit_live = Corpus(corpus_id='reddit_live999', storage_type='db')\n",
    "print(\"ID: \" + str(reddit_live.id))\n",
    "print([utt.id for utt in reddit_live.iter_utterances()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to corpus reddit_live999_v0 in place in the DB\n",
      "init db has utts ['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq']\n",
      "Old Id: reddit_live999\n",
      "No filename or corpus name specified for DB storage; using name 193735\n",
      "Corpus 193735_v0 not found in the DB; building new corpus, in place.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running _merge_utterances\n",
      "No filename or corpus name specified for DB storage; using name 756338\n",
      "Corpus 756338_v0 not found in the DB; building new corpus, in place.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:00<00:00, 14.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Id: reddit_live999\n",
      "['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq', 'i4ql5sh']\n",
      "['i4ql5sh']\n",
      "Connecting to corpus reddit_live999_v0 in place in the DB\n",
      "end db has utts ['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq', 'i4ql5sh']\n",
      "Finished One Round\n",
      "\n",
      "\n",
      "Connecting to corpus reddit_live999_v0 in place in the DB\n",
      "init db has utts ['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq', 'i4ql5sh']\n",
      "Old Id: reddit_live999\n",
      "No filename or corpus name specified for DB storage; using name 801805\n",
      "Corpus 801805_v0 not found in the DB; building new corpus, in place.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running _merge_utterances\n",
      "No filename or corpus name specified for DB storage; using name 899333\n",
      "Corpus 899333_v0 not found in the DB; building new corpus, in place.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 17.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Id: reddit_live999\n",
      "['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq', 'i4ql5sh', 'i4ql5zn']\n",
      "['i4ql5sh', 'i4ql5zn']\n",
      "Connecting to corpus reddit_live999_v0 in place in the DB\n",
      "end db has utts ['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq', 'i4ql5sh', 'i4ql5zn']\n",
      "Finished One Round\n",
      "\n",
      "\n",
      "Connecting to corpus reddit_live999_v0 in place in the DB\n",
      "init db has utts ['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq', 'i4ql5sh', 'i4ql5zn']\n",
      "Old Id: reddit_live999\n",
      "No filename or corpus name specified for DB storage; using name 763150\n",
      "Corpus 763150_v0 not found in the DB; building new corpus, in place.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running _merge_utterances\n",
      "No filename or corpus name specified for DB storage; using name 312132\n",
      "Corpus 312132_v0 not found in the DB; building new corpus, in place.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 21.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Id: reddit_live999\n",
      "['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq', 'i4ql5sh', 'i4ql5zn', 'i4ql6c2']\n",
      "['i4ql5sh', 'i4ql5zn', 'i4ql6c2']\n",
      "Connecting to corpus reddit_live999_v0 in place in the DB\n",
      "end db has utts ['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq', 'i4ql5sh', 'i4ql5zn', 'i4ql6c2']\n",
      "Finished One Round\n",
      "\n",
      "\n",
      "Connecting to corpus reddit_live999_v0 in place in the DB\n",
      "init db has utts ['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq', 'i4ql5sh', 'i4ql5zn', 'i4ql6c2']\n",
      "Old Id: reddit_live999\n",
      "No filename or corpus name specified for DB storage; using name 138899\n",
      "Corpus 138899_v0 not found in the DB; building new corpus, in place.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running _merge_utterances\n",
      "No filename or corpus name specified for DB storage; using name 465041\n",
      "Corpus 465041_v0 not found in the DB; building new corpus, in place.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 20.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Id: reddit_live999\n",
      "['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq', 'i4ql5sh', 'i4ql5zn', 'i4ql6c2', 'i4ql6yz']\n",
      "['i4ql5sh', 'i4ql5zn', 'i4ql6c2', 'i4ql6yz']\n",
      "Connecting to corpus reddit_live999_v0 in place in the DB\n",
      "end db has utts ['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq', 'i4ql5sh', 'i4ql5zn', 'i4ql6c2', 'i4ql6yz']\n",
      "Finished One Round\n",
      "\n",
      "\n",
      "Connecting to corpus reddit_live999_v0 in place in the DB\n",
      "init db has utts ['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq', 'i4ql5sh', 'i4ql5zn', 'i4ql6c2', 'i4ql6yz']\n",
      "Old Id: reddit_live999\n",
      "No filename or corpus name specified for DB storage; using name 112380\n",
      "Corpus 112380_v0 not found in the DB; building new corpus, in place.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running _merge_utterances\n",
      "No filename or corpus name specified for DB storage; using name 823715\n",
      "Corpus 823715_v0 not found in the DB; building new corpus, in place.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 20.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Id: reddit_live999\n",
      "['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq', 'i4ql5sh', 'i4ql5zn', 'i4ql6c2', 'i4ql6yz', 'i4ql76n']\n",
      "['i4ql5sh', 'i4ql5zn', 'i4ql6c2', 'i4ql6yz', 'i4ql76n']\n",
      "Connecting to corpus reddit_live999_v0 in place in the DB\n",
      "end db has utts ['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq', 'i4ql5sh', 'i4ql5zn', 'i4ql6c2', 'i4ql6yz', 'i4ql76n']\n",
      "Finished One Round\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "ids = []\n",
    "last_corpus_id=None\n",
    "for comment in reddit.subreddit('funny').stream.comments(skip_existing=True):\n",
    "    reddit_db = Corpus(corpus_id=\"reddit_live999\", storage_type='db', in_place=True)\n",
    "    print('init db has utts',[a.id for a in reddit_db.iter_utterances()])\n",
    "    utt = Utterance(id=comment.id,\n",
    "                    text=comment.body,\n",
    "                    reply_to=comment.parent_id.split('_')[1],\n",
    "                    speaker=Speaker(\n",
    "                        id=comment.author.name if comment.author is not None else \"n/a\",),\n",
    "                    conversation_id=comment.submission.id,\n",
    "                    timestamp=comment.created_utc)\n",
    "    ids.append(utt.id) # Will use this external list of ids for a check in the next cell.\n",
    "    reddit_live = reddit_live.add_utterances([utt])\n",
    "    reddit_db = Corpus(corpus_id=\"reddit_live999\", storage_type='db', in_place=True)\n",
    "    print('end db has utts',[c.id for c in reddit_db.iter_utterances()])\n",
    "    print(\"Finished One Round\\n\\n\")\n",
    "    sleep(1)\n",
    "    c += 1\n",
    "    if c >= 5:\n",
    "        # Simulating a server crash after 5 iterations\n",
    "        del reddit_live\n",
    "        break \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, with no dump necessary, the data is already stored persistently in the database despite the crash. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to corpus reddit_live999_v0 in place in the DB\n",
      "still has utts ['i4qkjzo', 'i4qkkmr', 'i4qkloa', 'i4qkloi', 'i4qknpq', 'i4ql5sh', 'i4ql5zn', 'i4ql6c2', 'i4ql6yz', 'i4ql76n']\n",
      "Pakmanjosh: 8xl size exists!?\n",
      "------------------------------\n",
      "blackbeltbud: As someone who works for one of their competitors, I enjoy seeing this stuff lmao. Not necessarily cause they've failed, just cool seeing something from my industry on the front page. Everyone uses us, no one talks about it.\n",
      "------------------------------\n",
      "biddilybong: The girl is a smoke show but dad has all the moves.\n",
      "------------------------------\n",
      "DryCalligrapher8696: Nice one!\n",
      "------------------------------\n",
      "Good-Helicopter-9303: Where did you get those cool sunglasses from? I like them.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "reddit_live_2 = Corpus(corpus_id=\"reddit_live999\", storage_type='db', in_place=True)\n",
    "print('still has utts',[utt.id for utt in reddit_live_2.iter_utterances()])\n",
    "for id in ids:\n",
    "    assert reddit_live_2.has_utterance(id)\n",
    "    utterance = reddit_live_2.get_utterance(id)\n",
    "    print(f'{utterance.speaker.id}: {utterance.text}')\n",
    "    print('------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}